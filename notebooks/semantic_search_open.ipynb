{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, get_embeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use the PyPDF2 library to read a PDF file\n",
    "from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINE = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"semantic-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"multi-qa-mpnet-base-cos-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"/tmp/semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    "    embedding_function=sentence_transformer_ef\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ae76cc4dfd345ecaeea9b8ba0d5c3437'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_chroma(texts, engine=None):\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    if engine:\n",
    "        embeddings = get_embeddings(texts, engine=ENGINE)\n",
    "        return {\n",
    "        'ids':[my_hash(text) for text in texts],\n",
    "        'documents': [text for text in texts],\n",
    "        'embeddings': [embedding for embedding in embeddings],\n",
    "        'metadata': [dict(head=text[0], date_uploaded=str(now)) for text in texts]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'ids':[my_hash(text) for text in texts],\n",
    "        'documents': [text for text in texts],\n",
    "        'metadata': [dict(head=text[0], date_uploaded=str(now)) for text in texts]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  prepare_for_chroma(texts, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['49f68a5c8493ec2c0bf489821c21fc3b'],\n",
       " 'documents': ['hi'],\n",
       " 'metadata': [{'head': 'h', 'date_uploaded': '2023-10-20 13:24:57.880216'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_chroma(texts, collection, batch_size=None, show_progress_bar=True, engine=None):\n",
    "    total_added = 0\n",
    "    if not batch_size:\n",
    "        batch_size = len(texts)\n",
    "\n",
    "    _range = range(0, len(texts), batch_size)\n",
    "    for i in tqdm(_range) if show_progress_bar else _range:\n",
    "        batch = texts[i : i + batch_size]\n",
    "        output = prepare_for_chroma(batch, engine=engine)\n",
    "\n",
    "        if output.get('embeddings', None):\n",
    "            out = collection.add(\n",
    "                documents= output['documents'],\n",
    "                embeddings= output['embeddings'],\n",
    "                metadatas= output['metadata'],\n",
    "                ids= output['ids']\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            out = collection.add(\n",
    "                documents= output['documents'],\n",
    "                metadatas= output['metadata'],\n",
    "                ids= output['ids']\n",
    "                )\n",
    "        print(out)\n",
    "        total_added += 1\n",
    "\n",
    "        return total_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_texts_to_chroma(texts, collection, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_chroma(query, collection, engine=None, top_k=3):\n",
    "    if engine:\n",
    "        query_embedding = get_embedding(query, engine=ENGINE)\n",
    "\n",
    "        return collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=top_k\n",
    "            )\n",
    "    return collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=top_k\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['49f68a5c8493ec2c0bf489821c21fc3b']],\n",
       " 'distances': [[0.07519697162713412]],\n",
       " 'metadatas': [[{'date_uploaded': '2023-10-18 09:35:59.505288'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['hi']]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_from_chroma('hello', collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_texts_from_chroma(texts, collection):\n",
    "    hashes = [my_hash(text) for text in texts]\n",
    "\n",
    "    return collection.delete(\n",
    "        ids=hashes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete text\n",
    "delete_texts_from_chroma(texts, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test collection is empty\n",
    "query_from_chroma('hello', collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Source Embedding Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out the bi encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814705014228821 Around 9 Million people live in London\n",
      "0.5050859451293945 London is known for its financial district\n"
     ]
    }
   ],
   "source": [
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Load the model\n",
    "# Initializing a SentenceTransformer model with the 'multi-qa-mpnet-base-cos-v1' pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(query)\n",
    "doc_emb = model.encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom delimiter approach has 30 documents with average length 768.0 tokens\n"
     ]
    }
   ],
   "source": [
    "# Only keep documents of at least 50 characters split by a custom delimiter\n",
    "split = list(filter(lambda x: len(x) > 50, intro_to_kube.split('\\n\\n\\n')))\n",
    "\n",
    "avg_length = sum([len(model.encode(t)) for t in split]) / len(split)\n",
    "print(f'custom delimiter approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:41<00:00, 41.66s/it]\n"
     ]
    }
   ],
   "source": [
    "corpus_embeddings = model.encode(split, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: 8 embeddings\n",
      "Cluster 1: 3 embeddings\n",
      "Cluster 2: 3 embeddings\n",
      "Cluster 3: 2 embeddings\n",
      "Cluster 4: 2 embeddings\n",
      "Cluster 5: 1 embeddings\n",
      "Cluster 6: 3 embeddings\n",
      "Cluster 7: 1 embeddings\n",
      "Cluster 8: 4 embeddings\n",
      "Cluster 9: 1 embeddings\n",
      "Cluster 10: 1 embeddings\n",
      "Cluster 11: 1 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Normalize the embeddings to unit length\n",
    "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Perform kmean clustering\n",
    "clustering_model = AgglomerativeClustering(n_clusters=None, metric='cosine', linkage='average', distance_threshold=0.4)\n",
    "clustering_model.fit(corpus_embeddings)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "# Print the number of embeddings in each cluster\n",
    "unique_labels, counts = np.unique(cluster_assignment, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f'Cluster {label}: {count} embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_documents = []\n",
    "for _label, count in zip(unique_labels, counts):\n",
    "    pruned_documents.append('\\n\\n'.join([text for text, label in zip(split, cluster_assignment) if label == _label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course: Introduction to Kubernetes L1:Introduction In this course, we will explore what kubernetes is; its architecture and building blocks, how it can be run on our local system or in the cloud, different ways we can configure and protect sensitive information, and how we can let external applications access our kubernetes application. We will also learn how to deploy and manage applications and resources with kubernetes. Here is a guide on how to create a GCP/Azure account Before diving into Kubernetes, we need to know some fundamental services that make working with Kubernetes easy and understandable. Containers Containers are an application-centric method used to deliver high-performing, scalable applications on any infrastructure of your choice. Containers provide a portable, isolated way of deploying microservices without disturbance from other microservices in our application. These containers install all the other dependencies needed by the microservice to function in a virtual environment. Containers are responsible for running container images, they do not run the microservices, rather, they couple the microservices and their dependencies. Container Images An image bundles the application with its runtime, libraries, and other dependencies. This serves as an isolated environment that runs the application. The runtime is the programming language that the application is built with or runs on. A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a pod Microservices Microservices are simple, portable, and light applications written in major programming languages such as Rust, Go, Python, etc. Microservices have specific dependencies, libraries, and environmental requirements. Microservices must be deployed with their dependencies before they can successfully run.\n",
      "\n",
      "In practice, the most important characteristics of a running microservice architecture are ephemerality and scalability. By ephemerality, we mean that individual instances of microservices (containers) will come and go over time. This can be for various reasons (scaling, updates, node failure etc), but the end effect is the same — your containers will die. Scalability (more accurately horizontal scalability) means that if you need to service more traffic, your microservice can easily be scaled up simply by adding more instances. Container Orchestrators Container orchestrators are tools which group systems together to form clusters where containers' deployment and management is automated at scale, while meeting the requirements that follow: 1. Fault-tolerance 2. On-demand scalability 3. Optimal resource usage 4. Accessibility from the outside world 5. Seamless updates/rollbacks without any downtime 6. Auto-discovery to automatically discover and communicate with each other. Importance of container orchestrators\n",
      "\n",
      "● Group hosts together while creating a cluster ● Schedule containers to run on hosts in the cluster based on resources availability ● Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster ● Bind containers and storage resources ● Group sets of similar containers and bind them to load-balancing constructs and simplify access to containerized applications by creating a level of abstraction between the containers and the user ● Manage and optimize resource usage ● Allow for implementation of policies to secure access to applications running inside containers. Deploying Container Orchestrators Container orchestrators can be deployed in any infrastructure of our choice. We can deploy containers on bare metal, virtual machines, on-premise, on public and hybrid cloud. They can be installed on top of cloud with resources such as Google compute engine, AWS EC2, Docker Enterprise, IBM Cloud etc. as infrastructure-as-a-service (IAAS). Also, some cloud services provide managed container orchestration-as-a-service solution, these are container orchestration that are managed and hosted by these cloud providers such as Amazon Elastic Kubernetes Service (Amazon EKS), Azure Kubernetes Service (AKS), DigitalOcean Kubernetes , Google Kubernetes Engine (GKE), IBM Cloud Kubernetes Service , Oracle Container Engine for Kubernetes Introduction to Kubernetes Quiz Question 1 Which is not an importance of container orchestrators? a. Docker b. Schedule containers to run on hosts in the cluster based on resources availability c. Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster d. Bind containers and volume resource Question 2 Container orchestrators can be deployed on bare metal, virtual machines, on-premise, and on-cloud infrastructures a. True b. False Question 3\n",
      "\n",
      "Tools which group systems together to form clusters where containers' deployment and management is automated at scale are called a. DEployment b. Container c. Images d. Container orchestrators Question 4 What does a container image encapsulate? a. Represents a binary data that encapsulates an application and all its OS dependencies. b. Represents a binary data structure that encapsulates an application and all its binary dependencies c. Represents a binary data that encapsulates an application and all its Software dependencies. d. None of the above. Question 5 Simple, portable, and light applications written in major programming languages that also have specific dependencies, libraries, and environmental requirements are a. Microservices b. Containers c. Docker d. Images L2: Kubernetes Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes can be referred to as k8s(pronounced Kate’s), 8 refers to the 8 characters between k and s. Kubernetes is an open source project started by Google and written in the Go programming language. The Kubernetes project was donated to the Cloud Native Computing foundation by Google, and new versions are released every 3 months. Features of Kubernetes Kubernetes offer a wide variety of features for orchestrating containers including: 1. Automatic bin packing: Containers are scheduled based on constraints and needed resources, this helps in maximizing utilization without sacrificing availability. 2. Self Healing: Kubernetes ensures a new pod is created to replace an unhealthy or a dead pod.\n",
      "\n",
      "3. Horizontal scaling: Applications are scaled manually or automatically based on CPU or custom metrics utilization. 4. Service discovery and load balancing: Containers receive their own IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set. 5. Automated rollouts and rollbacks: Seamless roll out and roll back application updates and configuration changes, constantly monitoring application health to prevent downtimes. 6. Storage orchestration: Mounts software-defined storage solutions to containers from local storage, external cloud providers, distributed storage, and network storage systems. 7. Batch execution: Kubernetes supports batch execution, long-running jobs, and replacement for failed containers. Architecture Kubernetes contains the following components at a high level 1. One or more master nodes. This is part of the control plane. 2. One or more worker nodes Master node: The master node is responsible for providing a running environment for the control plane. This control plane manages the affairs of a Kubernetes cluster. The control plane also contains other agents with separate responsibilities, and they help with cluster management too. Communication between users and the k8s cluster is done through the following means; the application programming interface (API), command-line interface (CLI), and Web user interface (Web UI). All these act as a channel for the user to send the requests to the control plane managing the k8s clusters. The control plane should be running at all times because it is an important service. Also, a disruption might result in downtime which may affect businesses as a result of data loss.\n",
      "\n",
      "Etcd: This is a distributed key-value store which is responsible for holding the k8s cluster state, It can be configured on the master node or on its dedicated host, separating it from other agents in the control plane. This is done to prevent data loss. Components in the Master Node API server Coordinates all administrative tasks. It intercepts calls from the users, operators, and external agents. It reads the Kubernetes cluster current state from the etcd datastore during processing and updates the resulting state of the k8s cluster in the etcd datastore after a call execution. This is the only component in the master node that can communicate to the etcd datastore. It acts as an intermediary for other components in the control plane. The API server can be configured and customized, it also supports horizontal and additional custom secondary API servers. It is a configuration that transforms the primary API Server into a proxy, to all secondary custom API Servers, and routes all incoming RESTful calls to them based on custom-defined rules. Scheduler The scheduler is responsible for assigning workloads in the cluster. It does this by looking at the current state of the Kubernetes cluster and the requirements of the new object. The scheduler, through the API server, receives the cluster state data and new object requirement data, the scheduler uses this new information to find the node that fits the requirements of the new workload. Once this is done, the API server is notified, which then notifies other control plane agents. The scheduler is also configurable and customized using scheduling policies, plugins, and profiles. Controller Manager This controls the state of the Kubernetes cluster. Controllers continuously run, monitor, and compare the cluster's desired state with its current state, from the etcd datastore, via the API server. If there are any differences, they are corrected until they match the desired state. Worker Node A worker node provides a running environment for clients application. Worker nodes are controlled by the control plane agents running on the master nodes. Services such as pods run on the master node. A pod is the smallest scheduling unit in Kubernetes. It is the logical collection of one or more containers scheduled together. The collection can be started, stopped, or rescheduled together as a single unit of work. Pods are scheduled on worker nodes, where they find required compute, memory, and storage resources to run, and networking to talk to each other and the outside world. The worker node has the following runtime:\n",
      "\n",
      "1. Container runtime 2. Node Agent - Kubelet 3. Proxy - Kube-proxy 4. Addons for DNS, Dashboard user interface, cluster-level monitoring and logging. Container Runtime Kubernetes requires a container runtime to handle the management of a container runtime. The container runtime is located on the node where a pod and its containers are to be scheduled. Kubernetes supports many runtimes including docker, containerd, cri-o etc. Kubelet The kubelet is the primary \"node agent\" that runs on each node. It is a node agent that is located on each worker node in a cluster and has the capability to communicate with the control plane from the master node. It communicates with the API server to receive pod definition and works with the container runtime in a node to run the containers linked to a pod, it also monitors the health and resources of the pods. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider.\n",
      "\n",
      "Kube proxy This is a network agent that runs on each node and is responsible for dynamically updating the networking rules on the node as well as maintaining the networking rules. It forwards connection requests to the Pods. DNS DNS server that assigns DNS records to Kubernetes objects and resources. Dashboard A web-based user interface for managing Kubernetes clusters Monitoring Responsible for collecting cluster level container metrics and saving them to a central data store Logging Collects cluster level container logs and stores them in a central log for later analysis Kubernetes Quiz Question 1 The following are features of kubernetes except a. Horizontal scaling b. Automatic rollout and rollbacks c. Storage orchestrator d. Deployment Question 2\n",
      "\n",
      "The kubernetes worker node contains the following runtime except a. Node agents b. Kubelet c. Container d. Container runtime Question 3 This is a distributed key-value store which is responsible for holding the k8s cluster state a. Kubelet b. Pods c. Etcd d. Volume Question 4 What is etcd? a. Kubernetes uses etcd as a distributed key-value store for all of its data, including metadata and configuration data, and allows nodes in Kubernetes clusters to read and write data. b. Etcd is a distributed key-value pair for holding the kubernetes cluster stateful metadata. c. The etcd is responsible for distributing the key-value pairs across the kubernetes cluster . d. All of the above Question 5 We can collect cluster level container logs and store them in a central log for later analysis with a. Logging b. Monitoring c. Trace d. Error reporting L3: Installing Kubernetes and MiniKubes In this lesson we will learn about the various configurations of Kubernetes, cluster configuration options, infrastructure, tools, and their requirements for running a kubernetes cluster deployment. Also, we will learn about the requirements for installing minikube, ways to install minikube on different OS, how we can get a kubernetes cluster running on minikube, and how to access the kubernetes dashboard.\n",
      "\n",
      "R1: Installing Kubernetes Objective ● Define Kubernetes cluster configuration ● Understand infrastructure where kubernetes is installed Configuration There are many different cluster configurations that can be used when installing kubernetes. Some of the configurations are described below: 1. All in One single node installation: Both master and worker nodes components are installed and running on a single node. This is used during learning and testing, it is not advisable for production. 2. Single master and multi worker: Single master node managing multiple worker nodes. The single master node runs a multi-stacked etcd instance. 3. Single master with single node etcd and multi worker nodes: We have a single master node with an external etcd instance. This master node manages the multiple worker nodes. 4. Multi master and multi worker: This configuration is mainly used for high availability . Multi master nodes are configured with stacked etcd instances installed in each master node. It is recommended to install kubernetes on a multi host environment, with support for high availability control plane setups, and multiple worker nodes for client workload . Infrastructure We need to determine the infrastructure for installing kubernetes cluster. This is determined by the environment type e.g learning or production. We need to decide the following: 1. How we should setup kubernetes: On bare metal, public cloud, private or hybrid 2. OS type: Linux or Windows 3. Network solution Localhost Installation ● Minikube - single-node local kubernetes cluster, recommended for a learning environment deployed on a single host. ● Kind - multi-node kubernetes cluster deployed in docker containers, acting as kubernetes nodes, recommended for a learning environment. ● Docker Desktop - including a local kubernetes cluster for docker users. ● MicroK8s - local and cloud kubernetes cluster, from canonical. ● K3S - lightweight kubernetes cluster for local, cloud, edge, IoT deployments, from Rancher.\n",
      "\n",
      "On Premise Installation For on premises, kubernetes can be installed on VMs or bare metal On-premise VMs: K8s can be installed on VMs created via Vagrant, VMware vSphere, KVM, or a configuration tool in conjunction with a hypervisor. Bare Metal: K8s can be installed on premise bare metal hosted on top different os. Cloud Installation Kubernetes can be installed on almost any cloud production environment. A few of them are listed below. Hosted SolutionsWith Hosted Solutions, any given software is completely managed by the provider, while the user pays hosting and management charges. Popular vendors providing hosted solutions for Kubernetes are (listed in alphabetical order): Alibaba Cloud Container Service for Kubernetes (ACK) Amazon Elastic Kubernetes Service (EKS) Azure Kubernetes Service (AKS) DigitalOcean Kubernetes Google Kubernetes Engine (GKE) IBM Cloud Kubernetes Service Oracle Cloud Container Engine for Kubernetes (OKE) Turnkey Cloud Solutions Below are only a few of the Turnkey Cloud Solutions (listed in alphabetical order), to install kubernetes on an underlying IaaS platform such as: Alibaba Cloud Amazon AWS (AWS EC2) Google Compute Engine (GCE) IBM Cloud Private Microsoft Azure (AKS). Turnkey On-Premise SolutionsThe On-Premise solutions install kubernetes on secure internal private clouds: GKE On-Prem part of Google Cloud Anthos IBM Private Cloud\n",
      "\n",
      "R2: Installing Minikubes Requirements 1. 2 CPUs or more 2. 2GB free memory 3. 20GB free disk 4. Connection to the Internet 5. A type-2 hypervisor like Docker, Virtual Box, VMWare, Hyper-V, KVM etc. Linux Verify virtualization for linux by running the below command on your terminal A non empty output indicates support for virtualization Install VirtualBox hypervisor for linux\n",
      "\n",
      "1. Pods: This is the smallest and simplest unit of deployment in Kubernetes that represents a single instance of application. It is a logical collection of one or more containers. Containers in a pod acts as a single entity and can: ● Be scheduled together on the same host ● Share IP address assigned to the Pod ● Mount the same volume 2. Labels: these are key-value pairs attached to objects. They are used to monitor and organize objects in a cluster, and select subset of objects based on requirements. Labels are used by controllers to group together objects that have been decoupled. The image above uses two labels app and env, each of the labels can be used to group two of the pods above, and we can also select one of the pods by mentioning key:value pair. The image above uses two labels app and env, each of the labels can be used to group two of the pods above, and we can also select one of the pods by mentioning key:value pair. 3. Label Selectors : Used by controllers to select a subset of objects. There are two types of selectors supported by kubernetes. ● Equality based Selectors: match objects based on label keys:values, this is achieved by using equality sign =,== for equal or != for not equal. ● Set based selectors: Filters objects based on a set of values, we use in and notion for label values and exist/does not exist for label keys.\n",
      "\n",
      "4 . ReplicaSet : Create replicas of our deployment by running a multiple instance of the same container component. The replica set implements self-healing, manual scaling, or automatic scaling using autoscaler. It supports both equality based selectors, and set based selector. The replica set adds a new pod to a deployment when it detects an unhealthy pod. 5. Deployment : Deployments handles the creation, deletion, and pods update. A deployment creates a ReplicaSet, which then creates a Pod. The Deployment manages the ReplicaSet and Pods. Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node's controller manager, and as a controller it also ensures that the current state always matches the desired state. It allows for seamless application updates and rollbacks through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling. 6. Namespaces: Namespace is a unique virtual sub cluster in a kubernetes cluster. This namespace can be used to create resources and objects unique to the namespace. Namespaces are unique to teams using a single kubernetes cluster and resources that need to be isolated. Kubernetes creates four namespaces by default: ● Kube-system: Contains objects created by the kubernetes system. Mainly control plane agents ● Default: contains objects created by an administrator or developers, and are assigned objects by default unless another namespace was specified by the developer or administrator. ● Kube-public: An unsecured namespace, readable by anyone and used for exposing public information about the cluster. ● Kube-node-lease: holds node-lease objects used for node heartbeat data. The command to list all the Namespaces in a cluster is Practice Project This tutorial provides an introduction to managing applications with StatefulSets . It demonstrates how to create, delete, scale, and update the Pods of StatefulSets. stateful basic Build and deploy a simple, multi-tier web application using Kubernetes and Docker\n",
      "\n",
      "We will also discuss what volumes are, the different types of volumes, and how they help attach persistent volumes to pods. R1: Services Objective ● Define Services. ● Explore Kube proxy and Service Discovery. ● Discuss the different service discovery runtime. ● Discuss the different service type A service offers a single DNS entry for a containerized application managed by Kubernetes regardless of the number of replicas, by providing a common load balancing access point to a set of pods logically grouped, and managed by a controller such as Deployment, ReplicaSet, or DaemonSet. Service exposes single Pods, ReplicaSet, Deployments, DaemonSets, and StatefulSets. Service forwards traffic to Pods by using a specified target port or selecting the port which the service is receiving traffic. Service endpoints is a logical set of s Pod’s Ip address along with the target points 10.0.0.1.1:5000. These endpoints are created and managed automatically by the Service. Kube-proxy Kube-proxy is responsible for implementing the Service configuration on behalf of an administrator or developer, in order to enable traffic routing to an exposed application running in Pods. The kube-proxy keeps an eye on the master node for addition, updates, and removal of Services, and endpoints. Service Discovery It is important to be able to discover services at runtime. There are two methods supported by Kubernetes. 1. Environment Variable: A new set of environment variables are added in the Pods for all active service by the kubelet daemon running on that node, as soon as the Pod starts on any worker node. Services created after Pods are created will not have the environmental variables set in the Pods. 2. DNS: Kubernetes provides an add-on for DNS which creates a DNS record for each service, and has the format my-svc.my-namespace.svc.cluster .local. Services in the same namespace can locate other services with just their names Service Type\n",
      "\n",
      "1. ClusterIP: This is the default ServiceType, this is a virtual IP address received by a service. It is used to communicate with Services, and can only be accessed from within the cluster. 2. NodePort: a high-port, dynamically picked from the default range 30000-32767, is mapped to the respective Service, from all the worker nodes. This is in addition to the ClusterIP. We use this when we want our services to be reached from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. The Service is load balancing such requests, and only forwards the request to one of the Pods running the desired application. 3. LoadBalancer: in this service type, the external load balancer routes traffic to the NodePort and ClusterIP. The Service is exposed at a static port on each worker node and also exposed externally using an underlying cloud provider’s load balancer feature. If there is no support for the automatic creation of load balancer, then this service type won’t work. It’s field will be populated with the <Pending> status. 4. ExternalIP: A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform, or AWS, and a Load Balancer configured on the cloud provider's infrastructure. Service Practice Project Applications running in a Kubernetes cluster find and communicate with each other, and the outside world, through the Service abstraction. This document explains what happens to the source IP of packets sent to different types of Services, and how you can toggle this behavior according to your needs. Using Source Ip R2: Volumes Containers running in pods are ephemeral in nature. This means that once a pod crashes, all the data stored in a container are deleted. If a new pod is spinned up, it is brought up as a new pod without the old data. This is a challenge solved by the use of volumes. A volume is a mount point on the container’s file system backed by a storage system, the volume type determines the storage medium, content, and access mode. Containers running in a pod share the volume linked to that pod. The volume has the ability to outlive the containers of the pod, allowing for data preservation even though it is deleted once the pod is deleted.\n",
      "\n",
      "Volume Types The volume types determines the properties of the directory, this directory is mounted inside a pod and backed by the volume type. The properties can be size, content, access mode etc. 1. emptyDir: This is an empty volume created for the pod as soon as it is scheduled on the worker node. If the pod is terminated, everything in emptyDir is deleted forever. 2. hostPath: Shares a directory between the host and the Pod, if the pod is terminated, this volume type is not deleted with the pod, so everything in the hostPath is available to the host. 3. Nfs: mount an NFS share to a pod. 4. Secret: stores and pass sensitive information line passwords to a pod. 5. configMap: This volume type provides configuration data, shell commands, and arguments to a pod. 6. Iscsi: mount an iscsi share into a pod. 7. persistenV olumeClaim: attaches persistent volume to a pod We also have others like gcePersistentDisk, awsElasticBlockStore, azureDisk, azureFile. We mount these volume types to a pod. PersistentV olumes Persistent volumes are storage abstractions backed by several storage technologies, local to the host where Pods are deployed with its application containers, network attached storage, cloud storage. An administrator provisions the persistent volume. They are dynamically provisioned based on the StorageClass resource which contains predefined provisioners and parameters to create a PersistentVolume. Kubernetes provides a PersistentVolume subsystem which provides APIs to users and administrators, so they can manage and consume persistent storage. PersistentVolume API resource type is used to manage resources, while PersistentVolumeClaim API resource type is used to consume the persistent volume PersistentV olumeClaims This is a request for storage by a user. PersistentVolume resource request is done based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim. Once the user is done with the volume, the\n",
      "\n",
      "PersistentVolume will be released and used according to the persistentVolumeReclaimPolicy. PersistentV olume Practice Project MySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step. wordpress stateful app L7: ConfigMaps and Secrets In this lesson, we will learn about how we can pass configuration details and sensitive information, why configMaps and Secrets are important, and different ways we can create configMaps and Secrets. ConfigMaps and Secrets ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories. A ConfigMap can be created with the kubectl create command, and we can display its details using the kubectl get command. Create the ConfigMap Display the ConfigMap Details for my-config\n"
     ]
    }
   ],
   "source": [
    "print(pruned_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:16<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upload_texts_to_chroma(pruned_documents, collection, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I setup Kubernetes?\"\n",
    "\n",
    "results_from_chroma = query_from_chroma(query, collection, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['2c4f78d725df2535b7419e2bdc16082d',\n",
       "   'f22fe51da433105ca64c8495cb1f2468',\n",
       "   '41ac779329c19684696f020540501ff9',\n",
       "   '0811ea60dbb7b265bfee21243da17ebf',\n",
       "   '45c44600ec8fb03b99ff1ebe2ee6c81d']],\n",
       " 'distances': [[0.3146678855570244,\n",
       "   0.41500638989238825,\n",
       "   0.4536414574616634,\n",
       "   0.4635328474400209,\n",
       "   0.4721777930495821]],\n",
       " 'metadatas': [[{'date_uploaded': '2023-10-20 13:35:21.827338', 'head': 'C'},\n",
       "   {'date_uploaded': '2023-10-20 13:35:21.827338', 'head': 'C'},\n",
       "   {'date_uploaded': '2023-10-20 13:35:21.827338', 'head': 'C'},\n",
       "   {'date_uploaded': '2023-10-20 13:35:21.827338', 'head': 'I'},\n",
       "   {'date_uploaded': '2023-10-20 13:35:21.827338', 'head': 'W'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Course: Introduction to Kubernetes L1:Introduction In this course, we will explore what kubernetes is; its architecture and building blocks, how it can be run on our local system or in the cloud, different ways we can configure and protect sensitive information, and how we can let external applications access our kubernetes application. We will also learn how to deploy and manage applications and resources with kubernetes. Here is a guide on how to create a GCP/Azure account Before diving into Kubernetes, we need to know some fundamental services that make working with Kubernetes easy and understandable. Containers Containers are an application-centric method used to deliver high-performing, scalable applications on any infrastructure of your choice. Containers provide a portable, isolated way of deploying microservices without disturbance from other microservices in our application. These containers install all the other dependencies needed by the microservice to function in a virtual environment. Containers are responsible for running container images, they do not run the microservices, rather, they couple the microservices and their dependencies. Container Images An image bundles the application with its runtime, libraries, and other dependencies. This serves as an isolated environment that runs the application. The runtime is the programming language that the application is built with or runs on. A container image represents binary data that encapsulates an application and all its software dependencies. Container images are executable software bundles that can run standalone and that make very well defined assumptions about their runtime environment. You typically create a container image of your application and push it to a registry before referring to it in a pod Microservices Microservices are simple, portable, and light applications written in major programming languages such as Rust, Go, Python, etc. Microservices have specific dependencies, libraries, and environmental requirements. Microservices must be deployed with their dependencies before they can successfully run.\\n\\nIn practice, the most important characteristics of a running microservice architecture are ephemerality and scalability. By ephemerality, we mean that individual instances of microservices (containers) will come and go over time. This can be for various reasons (scaling, updates, node failure etc), but the end effect is the same — your containers will die. Scalability (more accurately horizontal scalability) means that if you need to service more traffic, your microservice can easily be scaled up simply by adding more instances. Container Orchestrators Container orchestrators are tools which group systems together to form clusters where containers\\' deployment and management is automated at scale, while meeting the requirements that follow: 1. Fault-tolerance 2. On-demand scalability 3. Optimal resource usage 4. Accessibility from the outside world 5. Seamless updates/rollbacks without any downtime 6. Auto-discovery to automatically discover and communicate with each other. Importance of container orchestrators\\n\\n● Group hosts together while creating a cluster ● Schedule containers to run on hosts in the cluster based on resources availability ● Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster ● Bind containers and storage resources ● Group sets of similar containers and bind them to load-balancing constructs and simplify access to containerized applications by creating a level of abstraction between the containers and the user ● Manage and optimize resource usage ● Allow for implementation of policies to secure access to applications running inside containers. Deploying Container Orchestrators Container orchestrators can be deployed in any infrastructure of our choice. We can deploy containers on bare metal, virtual machines, on-premise, on public and hybrid cloud. They can be installed on top of cloud with resources such as Google compute engine, AWS EC2, Docker Enterprise, IBM Cloud etc. as infrastructure-as-a-service (IAAS). Also, some cloud services provide managed container orchestration-as-a-service solution, these are container orchestration that are managed and hosted by these cloud providers such as Amazon Elastic Kubernetes Service (Amazon EKS), Azure Kubernetes Service (AKS), DigitalOcean Kubernetes , Google Kubernetes Engine (GKE), IBM Cloud Kubernetes Service , Oracle Container Engine for Kubernetes Introduction to Kubernetes Quiz Question 1 Which is not an importance of container orchestrators? a. Docker b. Schedule containers to run on hosts in the cluster based on resources availability c. Enable containers in a cluster to communicate with each other regardless of the host they are deployed to in the cluster d. Bind containers and volume resource Question 2 Container orchestrators can be deployed on bare metal, virtual machines, on-premise, and on-cloud infrastructures a. True b. False Question 3\\n\\nTools which group systems together to form clusters where containers\\' deployment and management is automated at scale are called a. DEployment b. Container c. Images d. Container orchestrators Question 4 What does a container image encapsulate? a. Represents a binary data that encapsulates an application and all its OS dependencies. b. Represents a binary data structure that encapsulates an application and all its binary dependencies c. Represents a binary data that encapsulates an application and all its Software dependencies. d. None of the above. Question 5 Simple, portable, and light applications written in major programming languages that also have specific dependencies, libraries, and environmental requirements are a. Microservices b. Containers c. Docker d. Images L2: Kubernetes Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Kubernetes can be referred to as k8s(pronounced Kate’s), 8 refers to the 8 characters between k and s. Kubernetes is an open source project started by Google and written in the Go programming language. The Kubernetes project was donated to the Cloud Native Computing foundation by Google, and new versions are released every 3 months. Features of Kubernetes Kubernetes offer a wide variety of features for orchestrating containers including: 1. Automatic bin packing: Containers are scheduled based on constraints and needed resources, this helps in maximizing utilization without sacrificing availability. 2. Self Healing: Kubernetes ensures a new pod is created to replace an unhealthy or a dead pod.\\n\\n3. Horizontal scaling: Applications are scaled manually or automatically based on CPU or custom metrics utilization. 4. Service discovery and load balancing: Containers receive their own IP addresses from Kubernetes, while it assigns a single Domain Name System (DNS) name to a set of containers to aid in load-balancing requests across the containers of the set. 5. Automated rollouts and rollbacks: Seamless roll out and roll back application updates and configuration changes, constantly monitoring application health to prevent downtimes. 6. Storage orchestration: Mounts software-defined storage solutions to containers from local storage, external cloud providers, distributed storage, and network storage systems. 7. Batch execution: Kubernetes supports batch execution, long-running jobs, and replacement for failed containers. Architecture Kubernetes contains the following components at a high level 1. One or more master nodes. This is part of the control plane. 2. One or more worker nodes Master node: The master node is responsible for providing a running environment for the control plane. This control plane manages the affairs of a Kubernetes cluster. The control plane also contains other agents with separate responsibilities, and they help with cluster management too. Communication between users and the k8s cluster is done through the following means; the application programming interface (API), command-line interface (CLI), and Web user interface (Web UI). All these act as a channel for the user to send the requests to the control plane managing the k8s clusters. The control plane should be running at all times because it is an important service. Also, a disruption might result in downtime which may affect businesses as a result of data loss.\\n\\nEtcd: This is a distributed key-value store which is responsible for holding the k8s cluster state, It can be configured on the master node or on its dedicated host, separating it from other agents in the control plane. This is done to prevent data loss. Components in the Master Node API server Coordinates all administrative tasks. It intercepts calls from the users, operators, and external agents. It reads the Kubernetes cluster current state from the etcd datastore during processing and updates the resulting state of the k8s cluster in the etcd datastore after a call execution. This is the only component in the master node that can communicate to the etcd datastore. It acts as an intermediary for other components in the control plane. The API server can be configured and customized, it also supports horizontal and additional custom secondary API servers. It is a configuration that transforms the primary API Server into a proxy, to all secondary custom API Servers, and routes all incoming RESTful calls to them based on custom-defined rules. Scheduler The scheduler is responsible for assigning workloads in the cluster. It does this by looking at the current state of the Kubernetes cluster and the requirements of the new object. The scheduler, through the API server, receives the cluster state data and new object requirement data, the scheduler uses this new information to find the node that fits the requirements of the new workload. Once this is done, the API server is notified, which then notifies other control plane agents. The scheduler is also configurable and customized using scheduling policies, plugins, and profiles. Controller Manager This controls the state of the Kubernetes cluster. Controllers continuously run, monitor, and compare the cluster\\'s desired state with its current state, from the etcd datastore, via the API server. If there are any differences, they are corrected until they match the desired state. Worker Node A worker node provides a running environment for clients application. Worker nodes are controlled by the control plane agents running on the master nodes. Services such as pods run on the master node. A pod is the smallest scheduling unit in Kubernetes. It is the logical collection of one or more containers scheduled together. The collection can be started, stopped, or rescheduled together as a single unit of work. Pods are scheduled on worker nodes, where they find required compute, memory, and storage resources to run, and networking to talk to each other and the outside world. The worker node has the following runtime:\\n\\n1. Container runtime 2. Node Agent - Kubelet 3. Proxy - Kube-proxy 4. Addons for DNS, Dashboard user interface, cluster-level monitoring and logging. Container Runtime Kubernetes requires a container runtime to handle the management of a container runtime. The container runtime is located on the node where a pod and its containers are to be scheduled. Kubernetes supports many runtimes including docker, containerd, cri-o etc. Kubelet The kubelet is the primary \"node agent\" that runs on each node. It is a node agent that is located on each worker node in a cluster and has the capability to communicate with the control plane from the master node. It communicates with the API server to receive pod definition and works with the container runtime in a node to run the containers linked to a pod, it also monitors the health and resources of the pods. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider.\\n\\nKube proxy This is a network agent that runs on each node and is responsible for dynamically updating the networking rules on the node as well as maintaining the networking rules. It forwards connection requests to the Pods. DNS DNS server that assigns DNS records to Kubernetes objects and resources. Dashboard A web-based user interface for managing Kubernetes clusters Monitoring Responsible for collecting cluster level container metrics and saving them to a central data store Logging Collects cluster level container logs and stores them in a central log for later analysis Kubernetes Quiz Question 1 The following are features of kubernetes except a. Horizontal scaling b. Automatic rollout and rollbacks c. Storage orchestrator d. Deployment Question 2\\n\\nThe kubernetes worker node contains the following runtime except a. Node agents b. Kubelet c. Container d. Container runtime Question 3 This is a distributed key-value store which is responsible for holding the k8s cluster state a. Kubelet b. Pods c. Etcd d. Volume Question 4 What is etcd? a. Kubernetes uses etcd as a distributed key-value store for all of its data, including metadata and configuration data, and allows nodes in Kubernetes clusters to read and write data. b. Etcd is a distributed key-value pair for holding the kubernetes cluster stateful metadata. c. The etcd is responsible for distributing the key-value pairs across the kubernetes cluster . d. All of the above Question 5 We can collect cluster level container logs and store them in a central log for later analysis with a. Logging b. Monitoring c. Trace d. Error reporting L3: Installing Kubernetes and MiniKubes In this lesson we will learn about the various configurations of Kubernetes, cluster configuration options, infrastructure, tools, and their requirements for running a kubernetes cluster deployment. Also, we will learn about the requirements for installing minikube, ways to install minikube on different OS, how we can get a kubernetes cluster running on minikube, and how to access the kubernetes dashboard.\\n\\nR1: Installing Kubernetes Objective ● Define Kubernetes cluster configuration ● Understand infrastructure where kubernetes is installed Configuration There are many different cluster configurations that can be used when installing kubernetes. Some of the configurations are described below: 1. All in One single node installation: Both master and worker nodes components are installed and running on a single node. This is used during learning and testing, it is not advisable for production. 2. Single master and multi worker: Single master node managing multiple worker nodes. The single master node runs a multi-stacked etcd instance. 3. Single master with single node etcd and multi worker nodes: We have a single master node with an external etcd instance. This master node manages the multiple worker nodes. 4. Multi master and multi worker: This configuration is mainly used for high availability . Multi master nodes are configured with stacked etcd instances installed in each master node. It is recommended to install kubernetes on a multi host environment, with support for high availability control plane setups, and multiple worker nodes for client workload . Infrastructure We need to determine the infrastructure for installing kubernetes cluster. This is determined by the environment type e.g learning or production. We need to decide the following: 1. How we should setup kubernetes: On bare metal, public cloud, private or hybrid 2. OS type: Linux or Windows 3. Network solution Localhost Installation ● Minikube - single-node local kubernetes cluster, recommended for a learning environment deployed on a single host. ● Kind - multi-node kubernetes cluster deployed in docker containers, acting as kubernetes nodes, recommended for a learning environment. ● Docker Desktop - including a local kubernetes cluster for docker users. ● MicroK8s - local and cloud kubernetes cluster, from canonical. ● K3S - lightweight kubernetes cluster for local, cloud, edge, IoT deployments, from Rancher.\\n\\nOn Premise Installation For on premises, kubernetes can be installed on VMs or bare metal On-premise VMs: K8s can be installed on VMs created via Vagrant, VMware vSphere, KVM, or a configuration tool in conjunction with a hypervisor. Bare Metal: K8s can be installed on premise bare metal hosted on top different os. Cloud Installation Kubernetes can be installed on almost any cloud production environment. A few of them are listed below. Hosted SolutionsWith Hosted Solutions, any given software is completely managed by the provider, while the user pays hosting and management charges. Popular vendors providing hosted solutions for Kubernetes are (listed in alphabetical order): Alibaba Cloud Container Service for Kubernetes (ACK) Amazon Elastic Kubernetes Service (EKS) Azure Kubernetes Service (AKS) DigitalOcean Kubernetes Google Kubernetes Engine (GKE) IBM Cloud Kubernetes Service Oracle Cloud Container Engine for Kubernetes (OKE) Turnkey Cloud Solutions Below are only a few of the Turnkey Cloud Solutions (listed in alphabetical order), to install kubernetes on an underlying IaaS platform such as: Alibaba Cloud Amazon AWS (AWS EC2) Google Compute Engine (GCE) IBM Cloud Private Microsoft Azure (AKS). Turnkey On-Premise SolutionsThe On-Premise solutions install kubernetes on secure internal private clouds: GKE On-Prem part of Google Cloud Anthos IBM Private Cloud\\n\\nR2: Installing Minikubes Requirements 1. 2 CPUs or more 2. 2GB free memory 3. 20GB free disk 4. Connection to the Internet 5. A type-2 hypervisor like Docker, Virtual Box, VMWare, Hyper-V, KVM etc. Linux Verify virtualization for linux by running the below command on your terminal A non empty output indicates support for virtualization Install VirtualBox hypervisor for linux\\n\\n1. Pods: This is the smallest and simplest unit of deployment in Kubernetes that represents a single instance of application. It is a logical collection of one or more containers. Containers in a pod acts as a single entity and can: ● Be scheduled together on the same host ● Share IP address assigned to the Pod ● Mount the same volume 2. Labels: these are key-value pairs attached to objects. They are used to monitor and organize objects in a cluster, and select subset of objects based on requirements. Labels are used by controllers to group together objects that have been decoupled. The image above uses two labels app and env, each of the labels can be used to group two of the pods above, and we can also select one of the pods by mentioning key:value pair. The image above uses two labels app and env, each of the labels can be used to group two of the pods above, and we can also select one of the pods by mentioning key:value pair. 3. Label Selectors : Used by controllers to select a subset of objects. There are two types of selectors supported by kubernetes. ● Equality based Selectors: match objects based on label keys:values, this is achieved by using equality sign =,== for equal or != for not equal. ● Set based selectors: Filters objects based on a set of values, we use in and notion for label values and exist/does not exist for label keys.\\n\\n4 . ReplicaSet : Create replicas of our deployment by running a multiple instance of the same container component. The replica set implements self-healing, manual scaling, or automatic scaling using autoscaler. It supports both equality based selectors, and set based selector. The replica set adds a new pod to a deployment when it detects an unhealthy pod. 5. Deployment : Deployments handles the creation, deletion, and pods update. A deployment creates a ReplicaSet, which then creates a Pod. The Deployment manages the ReplicaSet and Pods. Deployment objects provide declarative updates to Pods and ReplicaSets. The DeploymentController is part of the master node\\'s controller manager, and as a controller it also ensures that the current state always matches the desired state. It allows for seamless application updates and rollbacks through rollouts and rollbacks, and it directly manages its ReplicaSets for application scaling. 6. Namespaces: Namespace is a unique virtual sub cluster in a kubernetes cluster. This namespace can be used to create resources and objects unique to the namespace. Namespaces are unique to teams using a single kubernetes cluster and resources that need to be isolated. Kubernetes creates four namespaces by default: ● Kube-system: Contains objects created by the kubernetes system. Mainly control plane agents ● Default: contains objects created by an administrator or developers, and are assigned objects by default unless another namespace was specified by the developer or administrator. ● Kube-public: An unsecured namespace, readable by anyone and used for exposing public information about the cluster. ● Kube-node-lease: holds node-lease objects used for node heartbeat data. The command to list all the Namespaces in a cluster is Practice Project This tutorial provides an introduction to managing applications with StatefulSets . It demonstrates how to create, delete, scale, and update the Pods of StatefulSets. stateful basic Build and deploy a simple, multi-tier web application using Kubernetes and Docker\\n\\nWe will also discuss what volumes are, the different types of volumes, and how they help attach persistent volumes to pods. R1: Services Objective ● Define Services. ● Explore Kube proxy and Service Discovery. ● Discuss the different service discovery runtime. ● Discuss the different service type A service offers a single DNS entry for a containerized application managed by Kubernetes regardless of the number of replicas, by providing a common load balancing access point to a set of pods logically grouped, and managed by a controller such as Deployment, ReplicaSet, or DaemonSet. Service exposes single Pods, ReplicaSet, Deployments, DaemonSets, and StatefulSets. Service forwards traffic to Pods by using a specified target port or selecting the port which the service is receiving traffic. Service endpoints is a logical set of s Pod’s Ip address along with the target points 10.0.0.1.1:5000. These endpoints are created and managed automatically by the Service. Kube-proxy Kube-proxy is responsible for implementing the Service configuration on behalf of an administrator or developer, in order to enable traffic routing to an exposed application running in Pods. The kube-proxy keeps an eye on the master node for addition, updates, and removal of Services, and endpoints. Service Discovery It is important to be able to discover services at runtime. There are two methods supported by Kubernetes. 1. Environment Variable: A new set of environment variables are added in the Pods for all active service by the kubelet daemon running on that node, as soon as the Pod starts on any worker node. Services created after Pods are created will not have the environmental variables set in the Pods. 2. DNS: Kubernetes provides an add-on for DNS which creates a DNS record for each service, and has the format my-svc.my-namespace.svc.cluster .local. Services in the same namespace can locate other services with just their names Service Type\\n\\n1. ClusterIP: This is the default ServiceType, this is a virtual IP address received by a service. It is used to communicate with Services, and can only be accessed from within the cluster. 2. NodePort: a high-port, dynamically picked from the default range 30000-32767, is mapped to the respective Service, from all the worker nodes. This is in addition to the ClusterIP. We use this when we want our services to be reached from the external world. The end-user connects to any worker node on the specified high-port, which proxies the request internally to the ClusterIP of the Service, then the request is forwarded to the applications running inside the cluster. The Service is load balancing such requests, and only forwards the request to one of the Pods running the desired application. 3. LoadBalancer: in this service type, the external load balancer routes traffic to the NodePort and ClusterIP. The Service is exposed at a static port on each worker node and also exposed externally using an underlying cloud provider’s load balancer feature. If there is no support for the automatic creation of load balancer, then this service type won’t work. It’s field will be populated with the <Pending> status. 4. ExternalIP: A Service can be mapped to an ExternalIP address if it can route to one or more of the worker nodes. Traffic that is ingressed into the cluster with the ExternalIP (as destination IP) on the Service port, gets routed to one of the Service endpoints. This type of service requires an external cloud provider such as Google Cloud Platform, or AWS, and a Load Balancer configured on the cloud provider\\'s infrastructure. Service Practice Project Applications running in a Kubernetes cluster find and communicate with each other, and the outside world, through the Service abstraction. This document explains what happens to the source IP of packets sent to different types of Services, and how you can toggle this behavior according to your needs. Using Source Ip R2: Volumes Containers running in pods are ephemeral in nature. This means that once a pod crashes, all the data stored in a container are deleted. If a new pod is spinned up, it is brought up as a new pod without the old data. This is a challenge solved by the use of volumes. A volume is a mount point on the container’s file system backed by a storage system, the volume type determines the storage medium, content, and access mode. Containers running in a pod share the volume linked to that pod. The volume has the ability to outlive the containers of the pod, allowing for data preservation even though it is deleted once the pod is deleted.\\n\\nVolume Types The volume types determines the properties of the directory, this directory is mounted inside a pod and backed by the volume type. The properties can be size, content, access mode etc. 1. emptyDir: This is an empty volume created for the pod as soon as it is scheduled on the worker node. If the pod is terminated, everything in emptyDir is deleted forever. 2. hostPath: Shares a directory between the host and the Pod, if the pod is terminated, this volume type is not deleted with the pod, so everything in the hostPath is available to the host. 3. Nfs: mount an NFS share to a pod. 4. Secret: stores and pass sensitive information line passwords to a pod. 5. configMap: This volume type provides configuration data, shell commands, and arguments to a pod. 6. Iscsi: mount an iscsi share into a pod. 7. persistenV olumeClaim: attaches persistent volume to a pod We also have others like gcePersistentDisk, awsElasticBlockStore, azureDisk, azureFile. We mount these volume types to a pod. PersistentV olumes Persistent volumes are storage abstractions backed by several storage technologies, local to the host where Pods are deployed with its application containers, network attached storage, cloud storage. An administrator provisions the persistent volume. They are dynamically provisioned based on the StorageClass resource which contains predefined provisioners and parameters to create a PersistentVolume. Kubernetes provides a PersistentVolume subsystem which provides APIs to users and administrators, so they can manage and consume persistent storage. PersistentVolume API resource type is used to manage resources, while PersistentVolumeClaim API resource type is used to consume the persistent volume PersistentV olumeClaims This is a request for storage by a user. PersistentVolume resource request is done based on type, access mode, and size. There are three access modes: ReadWriteOnce (read-write by a single node), ReadOnlyMany (read-only by many nodes), and ReadWriteMany (read-write by many nodes). Once a suitable PersistentVolume is found, it is bound to a PersistentVolumeClaim. Once the user is done with the volume, the\\n\\nPersistentVolume will be released and used according to the persistentVolumeReclaimPolicy. PersistentV olume Practice Project MySQL and Wordpress each require a PersistentVolume to store data. Their PersistentVolumeClaims will be created at the deployment step. wordpress stateful app L7: ConfigMaps and Secrets In this lesson, we will learn about how we can pass configuration details and sensitive information, why configMaps and Secrets are important, and different ways we can create configMaps and Secrets. ConfigMaps and Secrets ConfigMaps allow us to decouple the configuration details from the container image. Using ConfigMaps, we pass configuration data as key-value pairs which are consumed by Pods or any other system components and controllers, in the form of environment variables, sets of commands and arguments, or volumes. We can create ConfigMaps from literal values, from configuration files, from one or more files or directories. A ConfigMap can be created with the kubectl create command, and we can display its details using the kubectl get command. Create the ConfigMap Display the ConfigMap Details for my-config',\n",
       "   \"Configure a Pod to Use a ConfigMap L8: Ingress In this lesson we will discuss Ingress, and how it can be configured to let our applications be accessed from external services. Ingress An ingress is a collection of rules that allow inbound connections to reach the cluster services. Ingress configures a layer 7 HTTP/HTTPS load balancer for services and provides the following ● TLS (Transport Layer Security) ● Name-based virtual hosting ● Fanout routing ● Load Balancing ● Custom rules. With Ingress, users do not connect directly to a Service. Users reach the Ingress endpoint, and, from there, the request is forwarded to the desired Service. You can see an example of a sample Ingress definition below:\\n\\nWe can also define Fanout Ingress rules, when requests to example.com/blue and example.com/green would be forwarded to webserver-blue-svc and webserver-green-svc, respectively: The Ingress resource does not do any request forwarding by itself, it merely accepts the definitions of traffic routing rules. The ingress is fulfilled by an Ingress Controller, which is a reverse proxy responsible for traffic routing, based on rules defined in the Ingress resource. Ingress Controller An Ingress Controller is an application watching the Master Node's API server for changes in the Ingress resources and updates the Layer 7 Load Balancer accordingly. Ingress Controllers are also known as Controllers, Ingress Proxy, Service Proxy, Revers Proxy, etc. Kubernetes supports an array of Ingress Controllers, and, if needed, we can also build our own. GCE L7 Load Balancer Controller and Nginx Ingress Controller are commonly used Ingress Controllers. Other controllers are Contour , HAProxy Ingress , Istio , Kong , Traefik , etc. Start the Ingress Controller with Minikube Minikube ships with the Nginx Ingress Controller setup as an add-on, disabled by default. It can be easily enabled by running the following command:\\n\\nDeploy an Ingress Controller Once the Ingress Controller is deployed, we can create an Ingress resource using the kubectl create command. For example, if we create a virtual-host-ingress.yaml file with the Name-Based Virtual Hosting Ingress rule definition that we saw in the Ingress II section, then we use the following command to create an Ingress resource: Access Services Using Ingress As our current setup is on Minikube, we will need to update the host configuration file (/etc/hosts on Mac and Linux) on our workstation to the Minikube IP for those URLs. After the update, the file should look similar to: Now we can open blue.example.com and green.example.com on the browser, and access each application. Ingress Project\",\n",
       "   'Create a Kubernetes Service object that exposes an external IP address. Expose external Ip address',\n",
       "   'Installing kubectl Linux Installation Download latest stable kubectl binary, make it executable and move it to the PATH: Mac OS installation Download latest stable kubectl binary, make it executable and move it to the PATH: To install kubectl with Homebrew package manager , issue the following command:',\n",
       "   \"We can stop minikube with Minikube stop Accessing Kubernetes Kubernetes cluster can be accessed using the following methods 1. Command Line Interface (CLI) 2. Web-based User Interface. 3. APIs from CLI or programmatically . CLI Kubectl is the only kubernetes command line interface client that is used to manage cluster resources and applications. It is easily integrated into other systems, can be used in a script or for automation. Kubectl has to be configured with the credentials of the kubernetes cluster before it can be used. Web-based UI Kubernetes has a web based UI that is used to manage, monitor, and containerize applications and resources in a cluster. API server This is the main component of the kubernetes control plane, its responsibility is exposing the kubernetes APIs. These APIs allow users to interact with clusters directly. The CLI tools and Dashboard UI can be used to access the API server and perform various operations on the nodes.\\n\\nKube config file In order for kubectl to access the kubernetes cluster, it needs the master node endpoint and other credentials to communicate with the API server running on the master node. To do this, we need a config file stored inside the .kube directory. This is done automatically once minikube is started, but might need to be enabled in other tools. There can also be multiple kubeconfig files connected to a single kubectl client. The kubeconfig detail can be found in either the\\n\\nThe kubeconfig includes the API server's endpoint server: https://192.168.99.100:8443 and the minikube user's client authentication key and certificate data. Once kubectl is installed, we can display information about the Minikube Kubernetes cluster with the kubectl cluster-info command: Accessing Dashboard from minikube We can use the minikube dashboard command to access the kubernetes dashboard\\n\\nAccessing dashboard with kubectl proxy The kubectl proxy command authenticates with the API server on the master node to display the kubernetes dashboard on a different url, usually on the default proxy port 8001. We can do this with the kubectl proxy We can open another terminal and send requests to API on the default port 8001, without worrying about authentication. We can use the curl command to achieve this. To see all the endpoints for the API server we do this: curl http://localhost:8001/ Minikube Practice Project This tutorial shows you how to run a sample app on Kubernetes using minikube and Katacoda. Katacoda provides a free, in-browser Kubernetes environment. Hello minikube L4: Building Blocks of Kubernetes In this lesson we will explore the major building blocks of kubernetes, discuss Labels and Selectors; their roles in microservices and how they are used to group objects together. Building blocks of Kubernetes\"]]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_from_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

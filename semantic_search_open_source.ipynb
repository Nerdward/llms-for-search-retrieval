{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, get_embeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Use the PyPDF2 library to read a PDF file\n",
    "from pypdf import PdfReader\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGINE = 'text-embedding-ada-002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = get_embedding('I love to be vectorized', engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open-Source Embedding Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)e891a/.gitattributes: 100%|██████████| 737/737 [00:00<00:00, 5.29MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 1.30MB/s]\n",
      "Downloading (…)92a80e891a/README.md: 100%|██████████| 9.19k/9.19k [00:00<00:00, 31.7MB/s]\n",
      "Downloading (…)a80e891a/config.json: 100%|██████████| 571/571 [00:00<00:00, 2.08MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 833kB/s]\n",
      "Downloading (…)91a/data_config.json: 100%|██████████| 25.5k/25.5k [00:00<00:00, 60.0MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:01<00:00, 275MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 350kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 1.82MB/s]\n",
      "Downloading (…)e891a/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 66.5MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 2.83MB/s]\n",
      "Downloading (…)891a/train_script.py: 100%|██████████| 13.9k/13.9k [00:00<00:00, 28.1MB/s]\n",
      "Downloading (…)92a80e891a/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.55MB/s]\n",
      "Downloading (…)80e891a/modules.json: 100%|██████████| 349/349 [00:00<00:00, 1.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814705014228821 Around 9 Million people live in London\n",
      "0.5050859451293945 London is known for its financial district\n"
     ]
    }
   ],
   "source": [
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "#Load the model\n",
    "# Initializing a SentenceTransformer model with the 'multi-qa-mpnet-base-cos-v1' pre-trained model\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-cos-v1')\n",
    "\n",
    "#Encode query and documents\n",
    "query_emb = model.encode(query)\n",
    "doc_emb = model.encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = util.dot_score(query_emb, doc_emb)[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the PDF file in read-binary mode\n",
    "with open('', 'rb') as file:\n",
    "\n",
    "    # Create a PDF reader object\n",
    "    reader = PdfReader(file)\n",
    "\n",
    "    # Initialize an empty string to hold the text\n",
    "    principles_of_ds = ''\n",
    "\n",
    "    # Loop through each page in the PDF file\n",
    "    for page in tqdm(reader.pages):\n",
    "       \n",
    "        # Extract the text from the page\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Find the starting point of the text we want to extract\n",
    "        # In this case, we are extracting text starting from the string ' ]'\n",
    "        principles_of_ds += '\\n\\n' + text[text.find(' ]')+2:]\n",
    "\n",
    "# Strip any leading or trailing whitespace from the resulting string\n",
    "principles_of_ds = principles_of_ds.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36661, 1070]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the tiktoken library\n",
    "import tiktoken\n",
    "\n",
    "# Initializing a tokenizer for the 'cl100k_base' model\n",
    "# This tokenizer is designed to work with the 'ada-002' embedding model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Using the tokenizer to encode the text 'hey there'\n",
    "# The resulting output is a list of integers representing the encoded text\n",
    "# This is the input format required for embedding using the 'ada-002' model\n",
    "tokenizer.encode('hey there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to split the text into chunks of a maximum number of tokens. Inspired by OpenAI\n",
    "def overlapping_chunks(text, max_tokens = 500, overlapping_factor = 5):\n",
    "    '''\n",
    "    max_tokens: tokens we want per chunk\n",
    "    overlapping_factor: number of sentences to start each chunk with that overlaps with the previous chunk\n",
    "    '''\n",
    "\n",
    "    # Split the text using punctuation\n",
    "    sentences = re.split(r'[.?!]', text)\n",
    "\n",
    "    # Get the number of tokens for each sentence\n",
    "    n_tokens = [len(tokenizer.encode(\" \" + sentence)) for sentence in sentences]\n",
    "    \n",
    "    chunks, tokens_so_far, chunk = [], 0, []\n",
    "\n",
    "    # Loop through the sentences and tokens joined together in a tuple\n",
    "    for sentence, token in zip(sentences, n_tokens):\n",
    "\n",
    "        # If the number of tokens so far plus the number of tokens in the current sentence is greater \n",
    "        # than the max number of tokens, then add the chunk to the list of chunks and reset\n",
    "        # the chunk and tokens so far\n",
    "        if tokens_so_far + token > max_tokens:\n",
    "            chunks.append(\". \".join(chunk) + \".\")\n",
    "            if overlapping_factor > 0:\n",
    "                chunk = chunk[-overlapping_factor:]\n",
    "                tokens_so_far = sum([len(tokenizer.encode(c)) for c in chunk])\n",
    "            else:\n",
    "                chunk = []\n",
    "                tokens_so_far = 0\n",
    "\n",
    "        # If the number of tokens in the current sentence is greater than the max number of \n",
    "        # tokens, go to the next sentence\n",
    "        if token > max_tokens:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, add the sentence to the chunk and add the number of tokens to the total\n",
    "        chunk.append(sentence)\n",
    "        tokens_so_far += token + 1\n",
    "    if chunk:\n",
    "        chunks.append(\". \".join(chunk) + \".\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "#\n",
    "\n",
    "# A textbook about insects\n",
    "text = urlopen('https://www.gutenberg.org/cache/epub/10834/pg10834.txt').read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-overlapping chunking approach has 17 documents with average length 476.7 tokens\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(text, overlapping_factor=0)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'non-overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overlapping chunking approach has 24 documents with average length 477.4 tokens\n"
     ]
    }
   ],
   "source": [
    "split = overlapping_chunks(text)\n",
    "avg_length = sum([len(tokenizer.encode(t)) for t in split]) / len(split)\n",
    "print(f'overlapping chunking approach has {len(split)} documents with average length {avg_length:.1f} tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from datetime import datetime\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"semantic-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"/tmp/semantic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_or_create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ae76cc4dfd345ecaeea9b8ba0d5c3437'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_hash(s):\n",
    "    # Return the MD5 hash of the input string as a hexadecimal string\n",
    "    return hashlib.md5(s.encode()).hexdigest()\n",
    "\n",
    "my_hash('I love to hash it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_chroma(texts, engine=None):\n",
    "    now = datetime.utcnow()\n",
    "\n",
    "    if engine:\n",
    "        embeddings = get_embeddings(texts, engine=ENGINE)\n",
    "    \n",
    "    return {\n",
    "        'ids':[my_hash(text) for text in texts],\n",
    "        'documents': [text for text in texts],\n",
    "        'embeddings': [embedding for embedding in embeddings],\n",
    "        'metadata': {'date_uploaded':str(now)}\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  prepare_for_chroma(texts, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_texts_to_chroma(texts, collection, batch_size=None, show_progress_bar=True, engine=None):\n",
    "    total_added = 0\n",
    "    if not batch_size:\n",
    "        batch_size = len(texts)\n",
    "\n",
    "    _range = range(0, len(texts), batch_size)\n",
    "    for i in tqdm(_range) if show_progress_bar else _range:\n",
    "        batch = texts[i : i + batch_size]\n",
    "        output = prepare_for_chroma(batch, engine=engine)\n",
    "\n",
    "        out = collection.add(\n",
    "            documents= output['documents'],\n",
    "            embeddings= output['embeddings'],\n",
    "            metadatas= output['metadata'],\n",
    "            ids= output['ids']\n",
    "        )\n",
    "        \n",
    "        total_added += 1\n",
    "\n",
    "        return total_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Add of existing embedding ID: 49f68a5c8493ec2c0bf489821c21fc3b\n",
      "Insert of existing embedding ID: 49f68a5c8493ec2c0bf489821c21fc3b\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_texts_to_chroma(texts, collection, engine=ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_chroma(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
